{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadr as pyr\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sc\n",
    "import os\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from scipy.stats import chi2\n",
    "from sklearn.metrics import make_scorer\n",
    "from numpy.random import normal\n",
    "from sklearn.linear_model import Lasso\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from random import choices\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlens.ensemble import SuperLearner\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set directory:\n",
    "os.chdir(\"C:/Users/hp/OneDrive - Universidad de los Andes/Documentos/Docs/Universidad/2022-2/Big Data/Trabajo Final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"Base_2016_Clean.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Imputar missing values\n",
    "imputer=KNNImputer(n_neighbors=19,weights='distance')\n",
    "df_imp=imputer.fit_transform(df)\n",
    "X=pd.DataFrame(df_imp,columns=df.columns)\n",
    "Y=X[\"voto_ofrecido\"]\n",
    "X=X[[i for i in X.columns if i!=\"llaveper_n16\" and i!=\"voto_ofrecido\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\miniconda3\\lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py:336: FutureWarning: The parameter `n_jobs` has been deprecated in 0.10 and will be removed in 0.12. You can pass an nearest neighbors estimator where `n_jobs` is already set instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Train y test ####\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,Y,stratify=Y,test_size=0.2,random_state=911)\n",
    "##SMOTE para la train\n",
    "oversample = SMOTE(sampling_strategy=0.8,k_neighbors=19,random_state=911,n_jobs=-1)\n",
    "X_trains, y_trains = oversample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redondeo\n",
    "lista=['t_cuartos_hogar','t_personas', 'min_cabec','n_televisores', 'n_computadores', 'edad',\n",
    "'valor_arriendo_pagado', 'hor_salud', 'n_duchas', 'n_otros_bienes']\n",
    "for i in X_trains.columns:\n",
    "    if i not in lista:\n",
    "        X_trains[i]=X_trains[i].round(0)\n",
    "for i in X_train.columns:\n",
    "    if i not in lista:\n",
    "        X_train[i]=X_train[i].round(0)\n",
    "for i in X_test.columns:\n",
    "    if i not in lista:\n",
    "        X_test[i]=X_test[i].round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=(df[['voto_ofrecido','edad','sexo','zona', 'nivel_educ_1','nivel_educ_2','nivel_educ_3','nivel_educ_4','sp_estrato_2',\n",
    "'sp_estrato_3', 'sp_estrato_4', 'sp_estrato_5','sp_estrato_6','familias_accion', 'tenencia_vivienda_1',\n",
    " 'tenencia_vivienda_2', 'tenencia_vivienda_3', 'tenencia_vivienda_4',\n",
    " 'tenencia_vivienda_5','org_sindicato', \n",
    "'voto_alcaldia', 'evitar_iva']].describe(include=\"all\"))\n",
    "ds=ds.T\n",
    "ds=ds[[\"count\", \"mean\", \"std\", \"50%\"]]\n",
    "ds=ds.round(4)\n",
    "print(ds.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Metrica\n",
    "def wmir(y_true,y_pred):\n",
    "    CM = confusion_matrix(y_true, y_pred)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0]\n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    # False negative rate\n",
    "    FNR = FN/(TP+FN)\n",
    "    #False positive rate\n",
    "    FPR = FP/(FP+TN)\n",
    "    penalty=FNR*(2/3)+(1/3)*FPR\n",
    "    return penalty\n",
    "\n",
    "wmirs=make_scorer(wmir, greater_is_better=False)\n",
    "\n",
    "def FF(y_true,y_pred):\n",
    "    CM = confusion_matrix(y_true, y_pred)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0]\n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    # False negative rate\n",
    "    FNR = FN/(TP+FN)\n",
    "    FPR = FP/(FP+TN)\n",
    "    return FNR,FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn=KNeighborsClassifier()\n",
    "neighbors=[11,13,15,17,19,21,23,25,27,29,31]\n",
    "params = {\n",
    "    'n_neighbors': neighbors,\n",
    "    'weights': ['uniform','distance'],\n",
    "}\n",
    "knn_cv=GridSearchCV(knn,params,scoring=wmirs,n_jobs=-1, cv=10)\n",
    "re_knncv=knn_cv.fit(X_trains,y_trains.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 11, 'weights': 'distance'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_knncv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32088659639680045 (0.14423076923076922, 0.674198250728863)\n",
      "0.0 (0.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "print(wmir(y_test, re_knncv.predict(X_test)),FF(y_test, re_knncv.predict(X_test)))\n",
    "print(wmir(y_trains, re_knncv.predict(X_trains)),FF(y_trains, re_knncv.predict(X_trains))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solver': 'lbfgs', 'warm_start': True}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters={'solver':['newton-cg', 'lbfgs', 'sag', 'saga'],'warm_start': [True,False]}\n",
    "logistica=LogisticRegression(fit_intercept=True, penalty=\"none\", random_state=911, max_iter=1000,n_jobs=-1)\n",
    "logcv=GridSearchCV(logistica, parameters, scoring=wmirs)\n",
    "resultslogistic_b=logcv.fit(X_trains, y_trains.values.ravel())\n",
    "\n",
    "resultslogistic_b.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41224676683860356 (0.4951923076923077, 0.24635568513119532)\n",
      "0.3159230548711512 (0.3571753986332574, 0.23341836734693877)\n"
     ]
    }
   ],
   "source": [
    "print(wmir(y_test.values.ravel(),resultslogistic_b.predict(X_test)),FF(y_test.values.ravel(),resultslogistic_b.predict(X_test)))\n",
    "print(wmir(y_trains.values.ravel(),resultslogistic_b.predict(X_trains)),FF(y_trains.values.ravel(),resultslogistic_b.predict(X_trains)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\miniconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'reg_param': 0.020202020202020204}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "grilla=np.linspace(0,1,100)\n",
    "grilla=grilla.tolist()\n",
    "reg_param={\"reg_param\": grilla}\n",
    "qda=QuadraticDiscriminantAnalysis()\n",
    "qdacv=GridSearchCV(qda, reg_param, scoring=wmirs,n_jobs=-1, cv=10)\n",
    "modelqda=qdacv.fit(X_trains, y_trains.values.ravel())\n",
    "\n",
    "modelqda.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4259549973835688 (0.4807692307692308, 0.3163265306122449)\n",
      "0.13648363185169934 (0.06651480637813212, 0.2764212827988338)\n"
     ]
    }
   ],
   "source": [
    "print(wmir(y_test.values.ravel(),modelqda.predict(X_test)),FF(y_test.values.ravel(),modelqda.predict(X_test)))\n",
    "print(wmir(y_trains.values.ravel(),modelqda.predict(X_trains)),FF(y_trains.values.ravel(),modelqda.predict(X_trains)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\miniconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "qda2=QuadraticDiscriminantAnalysis(reg_param=0.425)\n",
    "modelqda2=qda2.fit(X_trains, y_trains.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34630335650743815 (0.25961538461538464, 0.5196793002915452)\n",
      "0.2370392999373521 (0.11503416856492027, 0.48104956268221577)\n"
     ]
    }
   ],
   "source": [
    "print(wmir(y_test.values.ravel(),modelqda2.predict(X_test)),FF(y_test.values.ravel(),modelqda2.predict(X_test)))\n",
    "print(wmir(y_trains.values.ravel(),modelqda2.predict(X_trains)),FF(y_trains.values.ravel(),modelqda2.predict(X_trains)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda=LinearDiscriminantAnalysis(solver='lsqr',shrinkage=0.525,n_components=1)\n",
    "modellda=lda.fit(X_trains, y_trains.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36543133737011285 (0.42788461538461536, 0.24052478134110788)\n",
      "0.33815520187899434 (0.38724373576309795, 0.23997813411078717)\n"
     ]
    }
   ],
   "source": [
    "print(wmir(y_test.values.ravel(),modellda.predict(X_test)),FF(y_test.values.ravel(),modellda.predict(X_test)))\n",
    "print(wmir(y_trains.values.ravel(),modellda.predict(X_trains)),FF(y_trains.values.ravel(),modellda.predict(X_trains)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "100 fits failed out of a total of 1000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hp\\miniconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 428, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"c:\\Users\\hp\\miniconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 275, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"c:\\Users\\hp\\miniconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 620, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\Users\\hp\\miniconda3\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\hp\\miniconda3\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\hp\\miniconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\hp\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 864, in _logistic_regression_path\n",
      "    w0, n_iter_i, warm_start_sag = sag_solver(\n",
      "  File \"c:\\Users\\hp\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py\", line 327, in sag_solver\n",
      "    num_seen, n_iter_ = sag(\n",
      "  File \"sklearn\\linear_model\\_sag_fast.pyx\", line 616, in sklearn.linear_model._sag_fast.sag64\n",
      "ValueError: Floating-point under-/overflow occurred at epoch #1. Scaling input data with StandardScaler or MinMaxScaler might help.\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hp\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\hp\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1589, in fit\n",
      "    fold_coefs_ = Parallel(\n",
      "  File \"c:\\Users\\hp\\miniconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1098, in __call__\n",
      "    self.retrieve()\n",
      "  File \"c:\\Users\\hp\\miniconda3\\lib\\site-packages\\joblib\\parallel.py\", line 975, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"c:\\Users\\hp\\miniconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 567, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"c:\\Users\\hp\\miniconda3\\lib\\concurrent\\futures\\_base.py\", line 445, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\Users\\hp\\miniconda3\\lib\\concurrent\\futures\\_base.py\", line 390, in __get_result\n",
      "    raise self._exception\n",
      "ValueError: Floating-point under-/overflow occurred at epoch #1. Scaling input data with StandardScaler or MinMaxScaler might help.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\hp\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [        nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan -0.32761568 -0.32761568 -0.32794975 -0.32794975\n",
      " -0.32837479 -0.32837479 -0.32855735 -0.32855735 -0.32870927 -0.32870927\n",
      " -0.32843576 -0.32843576 -0.32873939 -0.32873939 -0.32876968 -0.32876968\n",
      " -0.32901269 -0.32901269 -0.3297417  -0.3297417  -0.32779755 -0.32779755\n",
      " -0.32731188 -0.32731188 -0.32667407 -0.32667407 -0.32715996 -0.32715996\n",
      " -0.32691696 -0.32691696 -0.32785846 -0.32785846 -0.32773698 -0.32773698\n",
      " -0.32819264 -0.32819264 -0.32752457 -0.32752457 -0.32807126 -0.32807126\n",
      " -0.32646127 -0.32646127 -0.32709916 -0.32709916 -0.32649178 -0.32649178\n",
      " -0.32579339 -0.32579339 -0.32652226 -0.32652226 -0.32627926 -0.32627926\n",
      " -0.32661335 -0.32661335 -0.3271297  -0.3271297  -0.32694744 -0.32694744\n",
      " -0.32767643 -0.32767643 -0.32649161 -0.32649161 -0.32609694 -0.32609694\n",
      " -0.32633989 -0.32633989 -0.32658284 -0.32658284 -0.32637043 -0.32637043\n",
      " -0.32573264 -0.32573264 -0.32621854 -0.32621854 -0.32624886 -0.32624886\n",
      " -0.32609694 -0.32609694 -0.32637037 -0.32637037 -0.32640041 -0.32640041\n",
      " -0.32679539 -0.32679539 -0.32652215 -0.32652215 -0.32655247 -0.32655247\n",
      " -0.32700805 -0.32700805 -0.32649181 -0.32649181 -0.32615763 -0.32615763\n",
      " -0.32676524 -0.32676524 -0.32661335 -0.32661335 -0.32676521 -0.32676521\n",
      " -0.32703809 -0.32703809 -0.32700785 -0.32700785 -0.32646132 -0.32646132\n",
      " -0.32715988 -0.32715988 -0.32682581 -0.32682581 -0.32719039 -0.32719039\n",
      " -0.32658289 -0.32658289 -0.32618792 -0.32618792 -0.32643103 -0.32643103\n",
      " -0.32649183 -0.32649183 -0.32679508 -0.32679508 -0.3271596  -0.3271596\n",
      " -0.32661307 -0.32661307 -0.32661307 -0.32661307 -0.32731166 -0.32731166\n",
      " -0.32719025 -0.32719025 -0.32685624 -0.32685624 -0.32649175 -0.32649175\n",
      " -0.32627912 -0.32627912 -0.32618803 -0.32618803 -0.32706843 -0.32706843\n",
      " -0.3269166  -0.3269166  -0.32725075 -0.32725075 -0.32743309 -0.32743309\n",
      " -0.32734195 -0.32734195 -0.32764578 -0.32764578 -0.32731171 -0.32731171\n",
      " -0.32715991 -0.32715991 -0.32615768 -0.32615768 -0.32606649 -0.32606649\n",
      " -0.32712915 -0.32712915 -0.32712915 -0.32712915 -0.3271596  -0.3271596\n",
      " -0.32737224 -0.32737224 -0.32749381 -0.32749381 -0.32749386 -0.32749386\n",
      " -0.32779764 -0.32779764 -0.3274028  -0.3274028  -0.32676504 -0.32676504\n",
      " -0.32606654 -0.32606654]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 0.4444444444444444,\n",
       " 'l1_ratio': 0.5555555555555556,\n",
       " 'solver': 'saga',\n",
       " 'warm_start': True}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = np.linspace(0,1,10)\n",
    "C = np.linspace(0,1,10)\n",
    "parameters={'solver':['saga'],\n",
    "            'warm_start': [True,False],\n",
    "            'l1_ratio': l1,\n",
    "            'C': C\n",
    "            }\n",
    "elastic=LogisticRegression(fit_intercept=True, penalty=\"elasticnet\", random_state=911, max_iter=1000,n_jobs=-1)\n",
    "encv=GridSearchCV(elastic, parameters, scoring=wmirs)\n",
    "resultsen=encv.fit(X_trains, y_trains.values.ravel())\n",
    "\n",
    "resultsen.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4065653734021081 (0.4855769230769231, 0.24854227405247814)\n",
      "0.3194462523050222 (0.3630979498861048, 0.23214285714285715)\n"
     ]
    }
   ],
   "source": [
    "print(wmir(y_test.values.ravel(),resultsen.predict(X_test)),FF(y_test.values.ravel(),resultsen.predict(X_test)))\n",
    "print(wmir(y_trains.values.ravel(),resultsen.predict(X_trains)),FF(y_trains.values.ravel(),resultsen.predict(X_trains)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticnw=LogisticRegression(fit_intercept=True,l1_ratio=1,C=0.025, penalty=\"elasticnet\",solver='saga', random_state=911, max_iter=1000,n_jobs=-1)\n",
    "modelenw=elasticnw.fit(X_trains, y_trains.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3635344247589145 (0.4326923076923077, 0.22521865889212828)\n",
      "0.3485430211785332 (0.4123006833712984, 0.2210276967930029)\n"
     ]
    }
   ],
   "source": [
    "print(wmir(y_test.values.ravel(),modelenw.predict(X_test)),FF(y_test.values.ravel(),modelenw.predict(X_test)))\n",
    "print(wmir(y_trains.values.ravel(),modelenw.predict(X_trains)),FF(y_trains.values.ravel(),modelenw.predict(X_trains)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF=RandomForestClassifier(n_estimators=50,max_depth=2,criterion='entropy',class_weight={0: 1,1: 3.6},max_features=None,bootstrap=True,n_jobs=-1,random_state=911)\n",
    "resultsRF=RF.fit(X_trains, y_trains.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3244374672946101 (0.1346153846153846, 0.7040816326530612)\n",
      "0.2643676767810932 (0.04943052391799545, 0.6942419825072886)\n"
     ]
    }
   ],
   "source": [
    "print(wmir(y_test.values.ravel(),resultsRF.predict(X_test)),FF(y_test.values.ravel(),resultsRF.predict(X_test)))\n",
    "print(wmir(y_trains.values.ravel(),resultsRF.predict(X_trains)),FF(y_trains.values.ravel(),resultsRF.predict(X_trains)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "XGB=xgb.XGBRFClassifier(n_estimators=50,max_depth=2,scale_pos_weight=3.55,objective='binary:logistic',booster='gbtree',grow_policy='lossguide',n_jobs=-1,random_state=911)\n",
    "resultsXGB=XGB.fit(X_trains, y_trains.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32631569111160946 (0.052884615384615384, 0.8731778425655977)\n",
      "0.29637475068126823 (0.013895216400911162, 0.8613338192419825)\n"
     ]
    }
   ],
   "source": [
    "print(wmir(y_test.values.ravel(),resultsXGB.predict(X_test)),FF(y_test.values.ravel(),resultsXGB.predict(X_test)))\n",
    "print(wmir(y_trains.values.ravel(),resultsXGB.predict(X_trains)),FF(y_trains.values.ravel(),resultsXGB.predict(X_trains)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB2=xgb.XGBClassifier(n_estimators=35,max_depth=3,scale_pos_weight=16.75,reg_alpha=0,reg_lambda=1,objective='binary:logistic',booster='gbtree',grow_policy='depthwise',n_jobs=-1,random_state=911)\n",
    "resultsXGB2=XGB2.fit(X_trains, y_trains.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.315392090902295 (0.15384615384615385, 0.6384839650145773)\n",
      "0.20405147089750314 (0.003416856492027335, 0.6053206997084548)\n"
     ]
    }
   ],
   "source": [
    "print(wmir(y_test.values.ravel(),resultsXGB2.predict(X_test)),FF(y_test.values.ravel(),resultsXGB2.predict(X_test)))\n",
    "print(wmir(y_trains.values.ravel(),resultsXGB2.predict(X_trains)),FF(y_trains.values.ravel(),resultsXGB2.predict(X_trains)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.80004081\n",
      "Iteration 2, loss = 0.74373039\n",
      "Iteration 3, loss = 0.69466628\n",
      "Iteration 4, loss = 0.66457555\n",
      "Iteration 5, loss = 0.64645297\n",
      "Iteration 6, loss = 0.63446486\n",
      "Iteration 7, loss = 0.62376334\n",
      "Iteration 8, loss = 0.61568682\n",
      "Iteration 9, loss = 0.60971159\n",
      "Iteration 10, loss = 0.60361828\n",
      "Iteration 11, loss = 0.60040259\n",
      "Iteration 12, loss = 0.59316720\n",
      "Iteration 13, loss = 0.59003168\n",
      "Iteration 14, loss = 0.58573252\n",
      "Iteration 15, loss = 0.57939302\n",
      "Iteration 16, loss = 0.58224062\n",
      "Iteration 17, loss = 0.57436364\n",
      "Iteration 18, loss = 0.57035829\n",
      "Iteration 19, loss = 0.56819288\n",
      "Iteration 20, loss = 0.56632097\n",
      "Iteration 21, loss = 0.56593449\n",
      "Iteration 22, loss = 0.56100072\n",
      "Iteration 23, loss = 0.55827040\n",
      "Iteration 24, loss = 0.55869989\n",
      "Iteration 25, loss = 0.55761263\n",
      "Iteration 26, loss = 0.55600046\n",
      "Iteration 27, loss = 0.55556500\n",
      "Iteration 28, loss = 0.55318957\n",
      "Iteration 29, loss = 0.55478939\n",
      "Iteration 30, loss = 0.55502493\n",
      "Iteration 31, loss = 0.55226768\n",
      "Iteration 32, loss = 0.55084171\n",
      "Iteration 33, loss = 0.54890739\n",
      "Iteration 34, loss = 0.54965535\n",
      "Iteration 35, loss = 0.54773847\n",
      "Iteration 36, loss = 0.54723613\n",
      "Iteration 37, loss = 0.54801123\n",
      "Iteration 38, loss = 0.54900704\n",
      "Iteration 39, loss = 0.54843260\n",
      "Iteration 40, loss = 0.54443747\n",
      "Iteration 41, loss = 0.54660066\n",
      "Iteration 42, loss = 0.54547352\n",
      "Iteration 43, loss = 0.54343405\n",
      "Iteration 44, loss = 0.54494767\n",
      "Iteration 45, loss = 0.54440253\n",
      "Iteration 46, loss = 0.54495611\n",
      "Iteration 47, loss = 0.54524387\n",
      "Iteration 48, loss = 0.54364151\n",
      "Iteration 49, loss = 0.54366260\n",
      "Iteration 50, loss = 0.54119169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ANN=MLPClassifier(activation='relu',solver='adam',alpha=0.58,max_iter=50,verbose=True,hidden_layer_sizes=(2,4,8,16,32,64),random_state=911)\n",
    "resultsANN=ANN.fit(X_trains, y_trains.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37792479629214315 (0.41346153846153844, 0.30685131195335275)\n",
      "0.2287538380142164 (0.20318906605922551, 0.27988338192419826)\n"
     ]
    }
   ],
   "source": [
    "print(wmir(y_test.values.ravel(),resultsANN.predict(X_test)),FF(y_test.values.ravel(),resultsANN.predict(X_test)))\n",
    "print(wmir(y_trains.values.ravel(),resultsANN.predict(X_trains)),FF(y_trains.values.ravel(),resultsANN.predict(X_trains)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "param = {'num_leaves': 31, 'objective': 'binary'}\n",
    "train_data = lgb.Dataset(X_trains, label=y_trains)\n",
    "param['metric'] = 'auc'\n",
    "lgbclas=LGBMClassifier(n_estimators=106,scale_pos_weight=19,boosting_type='gbdt',reg_lambda=8 ,reg_alpha=9,num_leaves=9,max_depth=5,random_state=911,n_jobs=-1)\n",
    "re_lgb=lgbclas.fit(X_trains,y_trains,eval_metric=wmir)\n",
    "print(wmir(y_trains,re_lgb.predict(X_trains)))\n",
    "print(FF(y_trains,re_lgb.predict(X_trains)))\n",
    "print(wmir(y_test,re_lgb.predict(X_test)))\n",
    "print(FF(y_test,re_lgb.predict(X_test)))\n",
    "print(accuracy_score(y_test,re_lgb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons=[(2,4,8,16,32,64),(1,2,4,8,16,32)]\n",
    "activation=[('sigmoid','sigmoid','sigmoid','sigmoid','sigmoid','sigmoid'),('sigmoid','tanh','sigmoid','tanh','sigmoid','sigmoid'),\n",
    "    ('sigmoid','relu','sigmoid','relu','sigmoid','sigmoid'),\n",
    "    ('tanh','tanh','relu','tanh','tanh','tanh'),\n",
    "    ('sigmoid','sigmoid','softmax','sigmoid','sigmoid','sigmoid')]\n",
    "dropout=[(0.5,0.5,0.5,0.5,0.5),(0.1,0.1,0.1,0.1,0.1)]\n",
    "batch_size=[128]\n",
    "\n",
    "param_grid=dict(model_activation=activation,modelneurons=neurons,model_dropout=dropout, batch_size=batch_size)\n",
    "\n",
    "def model(neurons=(1,2,4,8,16,32), activation=('sigmoid','sigmoid','sigmoid','sigmoid','sigmoid','sigmoid'), dropout=(0.1,0.1,0.1,0.1,0.1)):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons[0], input_shape=(137,), activation=activation[0]))\n",
    "    model.add(Dense(neurons[1], activation=activation[1]))\n",
    "    model.add(Dropout(dropout[0]))\n",
    "    model.add(Dense(neurons[2], activation=activation[2]))\n",
    "    model.add(Dropout(dropout[1]))\n",
    "    model.add(Dense(neurons[3], activation=activation[3]))\n",
    "    model.add(Dropout(dropout[2]))\n",
    "    model.add(Dense(neurons[4], activation=activation[4]))\n",
    "    model.add(Dropout(dropout[3]))\n",
    "    model.add(Dense(neurons[5], activation=activation[5]))\n",
    "    model.add(Dropout(dropout[4]))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics='FalseNegatives')\n",
    "    return model\n",
    "modelo=KerasClassifier(model=model,epochs=80,verbose=1)\n",
    "grid=GridSearchCV(estimator=modelo, param_grid=param_grid, scoring=wmir, n_jobs=-1, cv=5, verbose=1)\n",
    "re_ann=grid.fit(X_trains,y_trains, class_weight={0: 1, 1: 2.8})\n",
    "print(re_ann.best_params_)\n",
    "y_pred=re_ann.predict(X_trains)\n",
    "y_pred2=re_ann.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79d71d161e7943240a345005223b4b57f09b9732a24e4917a9c0467b3aef16ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
